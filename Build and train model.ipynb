{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skeleton Code\n",
    "\n",
    "The code below provides a skeleton for the model building & training component of your project. You can add/remove/build on code however you see fit, this is meant as a starting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "from glob import glob\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain\n",
    "import sklearn.model_selection\n",
    "from random import sample \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, plot_precision_recall_curve, f1_score, confusion_matrix\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do some early processing of your metadata for easier model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Below is some helper code to read all of your full image filepaths into a dataframe for easier manipulation\n",
    "\n",
    "all_xray_df = pd.read_csv('/data/Data_Entry_2017.csv')\n",
    "all_image_paths = {os.path.basename(x): x for x in \n",
    "                   glob(os.path.join('/data','images*', '*', '*.png'))}\n",
    "print('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\n",
    "all_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n",
    "all_xray_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set unrealistic ages to NaN\n",
    "all_xray_df.replace(all_xray_df[all_xray_df['Patient Age']>100]['Patient Age'].values,np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here you may want to create some extra columns in your table with binary indicators of certain diseases \n",
    "## rather than working directly with the 'Finding Labels' column\n",
    "\n",
    "# Todo\n",
    "def split_labels(df):\n",
    "    labels = np.unique(list(chain(*df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\n",
    "    for i in labels:\n",
    "        df[i] = df['Finding Labels'].map(lambda y: 1.0 if i in y else 0)\n",
    "        \n",
    "split_labels(all_xray_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the 'Unnamed: 11' column from the dataset\n",
    "all_xray_df.drop('Unnamed: 11', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xray_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_xray_df['Pneumonia Class'] = all_xray_df['Pneumonia'].map(lambda x: 'Positive' if x == 1 else 'Negative')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_splits(vargs):\n",
    "    \n",
    "#Initial split\n",
    "    train_data, val_data = sklearn.model_selection.train_test_split(vargs, test_size = 0.2, stratify = vargs['Pneumonia'])\n",
    "\n",
    "#want equal number of +ve/-ve pneumonia cases in train set\n",
    "    train_p_inds = train_data[train_data.Pneumonia==1].index.tolist()\n",
    "    train_np_inds = train_data[train_data.Pneumonia==0].index.tolist()\n",
    "\n",
    "    train_np_sample = sample(train_np_inds,len(train_p_inds))\n",
    "    train_data = train_data.loc[train_p_inds + train_np_sample]\n",
    "\n",
    "#want % of +ve pneumonia cases in validation set to be equal to natural occurence of the disease in the main dataset\n",
    "    val_p_inds = val_data[val_data.Pneumonia==1].index.tolist()\n",
    "    val_np_inds = val_data[val_data.Pneumonia==0].index.tolist()\n",
    "\n",
    "# The following code pulls a random sample of non-pneumonia data that's 4 times as big as the pneumonia sample\n",
    "    val_np_sample = sample(val_np_inds, 4*len(val_p_inds))\n",
    "    val_data = val_data.loc[val_p_inds + val_np_sample]\n",
    "    return  train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the training and validation sets\n",
    "train_data, val_data = create_splits(all_xray_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANALYSE DISTRIBUTIONS IN TRAINING AND VALIDATION DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining age distribution fnc\n",
    "def age(df):\n",
    "    plt.hist(df['Patient Age'], bins = 10,)\n",
    "    plt.xlabel('age')\n",
    "    plt.ylabel('Number of People')\n",
    "    plt.title('Age Distribution in Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age distribution in training set\n",
    "age(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#majority of data is between 10 and 70 years of age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age distribution in validation set\n",
    "age(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to plot gender demographics\n",
    "def gender(df):\n",
    "    df['Patient Gender'].value_counts().plot(kind='bar')\n",
    "    plt.xlabel('Gender')\n",
    "    plt.ylabel('Number of People')\n",
    "    plt.title('Gender Distribution in Dataset')\n",
    "    \n",
    "    return df['Patient Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoking gender distribution fnc for train set\n",
    "train_gender_distribution = gender(train_data)\n",
    "train_gender_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoking gender distribution fnc for validation set\n",
    "val_gender_distribution = gender(val_data)\n",
    "val_gender_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fnc to show position distribution\n",
    "def image_pos(df):\n",
    "    df['View Position'].value_counts().plot(kind='bar')\n",
    "    plt.xlabel('Image Position')\n",
    "    plt.ylabel('Number of People')\n",
    "    plt.title('Image Position Distribution')\n",
    "    return df['View Position'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoking position distribution fnc for training set\n",
    "train_image_position_distribution = image_pos(train_data)\n",
    "train_image_position_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#invoking position distribution fnc for validation set\n",
    "val_image_position_distribution = image_pos(val_data)\n",
    "val_image_position_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the distribution of diseases comorbid with Pneumonia. Plotting the top 30 combinations\n",
    "train_data[train_data.Pneumonia == 1]['Finding Labels'].value_counts()[0:30].plot(kind = 'bar')\n",
    "plt.xlabel('Diseases Comorbid with Pneumonia')\n",
    "plt.ylabel('Number of People')\n",
    "plt.title('Distribution of Diseases comorbid with Pneumonia')\n",
    "train_disease_conjunction_pneumonia = train_data[train_data.Pneumonia == 1]['Finding Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the distribution of diseases comorbid with Pneumonia. Plotting the top 30 combinations\n",
    "val_data[val_data.Pneumonia == 1]['Finding Labels'].value_counts()[0:30].plot(kind = 'bar')\n",
    "plt.xlabel('Diseases Comorbid with Pneumonia')\n",
    "plt.ylabel('Number of People')\n",
    "plt.title('Distribution of Diseases comorbid with Pneumonia')\n",
    "val_disease_conjunction_pneumonia = val_data[val_data.Pneumonia == 1]['Finding Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the Pneumonia column since we have the Pneumonia Class column\n",
    "train_data.drop('Pneumonia', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the Pneumonia column since we have the Pneumonia Class column\n",
    "val_data.drop('Pneumonia', axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can begin our model-building & training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First suggestion: perform some image augmentation on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_image_augmentation(vargs):\n",
    "    \n",
    "    ## recommendation here to implement a package like Keras' ImageDataGenerator\n",
    "    ## with some of the built-in augmentations \n",
    "    \n",
    "    ## keep an eye out for types of augmentation that are or are not appropriate for medical imaging data\n",
    "    ## Also keep in mind what sort of augmentation is or is not appropriate for testing vs validation data\n",
    "    \n",
    "    ## STAND-OUT SUGGESTION: implement some of your own custom augmentation that's *not*\n",
    "    ## built into something like a Keras package\n",
    "    \n",
    "    # Todo\n",
    "    \n",
    "    return my_idg\n",
    "\n",
    "\n",
    "def make_train_gen(vargs):\n",
    "    \n",
    "    ## Create the actual generators using the output of my_image_augmentation for your training data\n",
    "    ## Suggestion here to use the flow_from_dataframe library, e.g.:\n",
    "    \n",
    "#     train_gen = my_train_idg.flow_from_dataframe(dataframe=train_df, \n",
    "#                                          directory=None, \n",
    "#                                          x_col = ,\n",
    "#                                          y_col = ,\n",
    "#                                          class_mode = 'binary',\n",
    "#                                          target_size = , \n",
    "#                                          batch_size = \n",
    "#                                          )\n",
    "     # Todo\n",
    "\n",
    "    return train_gen\n",
    "\n",
    "\n",
    "def make_val_gen(vargs):\n",
    "    \n",
    "#     val_gen = my_val_idg.flow_from_dataframe(dataframe = val_data, \n",
    "#                                              directory=None, \n",
    "#                                              x_col = ,\n",
    "#                                              y_col = ',\n",
    "#                                              class_mode = 'binary',\n",
    "#                                              target_size = , \n",
    "#                                              batch_size = ) \n",
    "    \n",
    "    # Todo\n",
    "    return val_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May want to pull a single large batch of random validation data for testing after each epoch:\n",
    "valX, valY = val_gen.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May want to look at some examples of our augmented training data. \n",
    "## This is helpful for understanding the extent to which data is being manipulated prior to training, \n",
    "## and can be compared with how the raw data look prior to augmentation\n",
    "\n",
    "t_x, t_y = next(train_gen)\n",
    "fig, m_axs = plt.subplots(4, 4, figsize = (16, 16))\n",
    "for (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n",
    "    c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "    if c_y == 1: \n",
    "        c_ax.set_title('Pneumonia')\n",
    "    else:\n",
    "        c_ax.set_title('No Pneumonia')\n",
    "    c_ax.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your model: \n",
    "\n",
    "Recommendation here to use a pre-trained network downloaded from Keras for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(vargs):\n",
    "    \n",
    "    # model = VGG16(include_top=True, weights='imagenet')\n",
    "    # transfer_layer = model.get_layer(lay_of_interest)\n",
    "    # vgg_model = Model(inputs = model.input, outputs = transfer_layer.output)\n",
    "    \n",
    "    # Todo\n",
    "    \n",
    "    return vgg_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_my_model(vargs):\n",
    "    \n",
    "    # my_model = Sequential()\n",
    "    # ....add your pre-trained model, and then whatever additional layers you think you might\n",
    "    # want for fine-tuning (Flatteen, Dense, Dropout, etc.)\n",
    "    \n",
    "    # if you want to compile your model within this function, consider which layers of your pre-trained model, \n",
    "    # you want to freeze before you compile \n",
    "    \n",
    "    # also make sure you set your optimizer, loss function, and metrics to monitor\n",
    "    \n",
    "    # Todo\n",
    "    \n",
    "    return my_model\n",
    "\n",
    "\n",
    "\n",
    "## STAND-OUT Suggestion: choose another output layer besides just the last classification layer of your modele\n",
    "## to output class activation maps to aid in clinical interpretation of your model's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Below is some helper code that will allow you to add checkpoints to your model,\n",
    "## This will save the 'best' version of your model by comparing it to previous epochs of training\n",
    "\n",
    "## Note that you need to choose which metric to monitor for your model's 'best' performance if using this code. \n",
    "## The 'patience' parameter is set to 10, meaning that your model will train for ten epochs without seeing\n",
    "## improvement before quitting\n",
    "\n",
    "# Todo\n",
    "\n",
    "# weight_path=\"{}_my_model.best.hdf5\".format('xray_class')\n",
    "\n",
    "# checkpoint = ModelCheckpoint(weight_path, \n",
    "#                              monitor= CHOOSE_METRIC_TO_MONITOR_FOR_PERFORMANCE, \n",
    "#                              verbose=1, \n",
    "#                              save_best_only=True, \n",
    "#                              mode= CHOOSE_MIN_OR_MAX_FOR_YOUR_METRIC, \n",
    "#                              save_weights_only = True)\n",
    "\n",
    "# early = EarlyStopping(monitor= SAME_AS_METRIC_CHOSEN_ABOVE, \n",
    "#                       mode= CHOOSE_MIN_OR_MAX_FOR_YOUR_METRIC, \n",
    "#                       patience=10)\n",
    "\n",
    "# callbacks_list = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train your model\n",
    "\n",
    "# Todo\n",
    "\n",
    "# history = my_model.fit_generator(train_gen, \n",
    "#                           validation_data = (valX, valY), \n",
    "#                           epochs = , \n",
    "#                           callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After training for some time, look at the performance of your model by plotting some performance statistics:\n",
    "\n",
    "Note, these figures will come in handy for your FDA documentation later in the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## After training, make some predictions to assess your model's overall performance\n",
    "## Note that detecting pneumonia is hard even for trained expert radiologists, \n",
    "## so there is no need to make the model perfect.\n",
    "my_model.load_weights(weight_path)\n",
    "pred_Y = new_model.predict(valX, batch_size = 32, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_auc(t_y, p_y):\n",
    "    \n",
    "    ## Hint: can use scikit-learn's built in functions here like roc_curve\n",
    "    \n",
    "    # Todo\n",
    "    \n",
    "    return\n",
    "\n",
    "## what other performance statistics do you want to include here besides AUC? \n",
    "\n",
    "\n",
    "# def ... \n",
    "# Todo\n",
    "\n",
    "# def ...\n",
    "# Todo\n",
    "    \n",
    "#Also consider plotting the history of your model training:\n",
    "\n",
    "def plot_history(history):\n",
    "    \n",
    "    # Todo\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot figures\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you feel you are done training, you'll need to decide the proper classification threshold that optimizes your model's performance for a given metric (e.g. accuracy, F1, precision, etc.  You decide) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find the threshold that optimize your model's performance,\n",
    "## and use that threshold to make binary classification. Make sure you take all your metrics into consideration.\n",
    "\n",
    "# Todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at some examples of predicted v. true with our best model: \n",
    "\n",
    "# Todo\n",
    "\n",
    "# fig, m_axs = plt.subplots(10, 10, figsize = (16, 16))\n",
    "# i = 0\n",
    "# for (c_x, c_y, c_ax) in zip(valX[0:100], testY[0:100], m_axs.flatten()):\n",
    "#     c_ax.imshow(c_x[:,:,0], cmap = 'bone')\n",
    "#     if c_y == 1: \n",
    "#         if pred_Y[i] > YOUR_THRESHOLD:\n",
    "#             c_ax.set_title('1, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('1, 0')\n",
    "#     else:\n",
    "#         if pred_Y[i] > YOUR_THRESHOLD: \n",
    "#             c_ax.set_title('0, 1')\n",
    "#         else:\n",
    "#             c_ax.set_title('0, 0')\n",
    "#     c_ax.axis('off')\n",
    "#     i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Just save model architecture to a .json:\n",
    "\n",
    "model_json = my_model.to_json()\n",
    "with open(\"my_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
